{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.distributions.binomial as binomial\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy\n",
    "\n",
    "from time import time # for time measurement\n",
    "\n",
    "plt.rc(\"figure\", dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.set_device(device)\n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "\n",
    "# # Reduce train & test sets for testing the procedure\n",
    "# train_dataset = torch.utils.data.Subset(train_dataset, list(range(6000)))\n",
    "# test_dataset = torch.utils.data.Subset(test_dataset, list(range(1000)))\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape, device=device) * std\n",
    "    w.requires_grad = True\n",
    "    \n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "\n",
    "def dropout(X, p_drop=0.5):\n",
    "    if not (0 < p_drop < 1): return X\n",
    "    phi = binomial.Binomial(torch.ones(X.shape), 1 - p_drop).sample()\n",
    "    \n",
    "    return phi.to(device)*X / (1-p_drop)\n",
    "\n",
    "\n",
    "def PRelu(X, a):\n",
    "    '''\n",
    "    Parametric ReLU, i.e. returns a copy of 'X' where all negative entries have been\n",
    "    multiplied with the corresponding entries in 'a'.\n",
    "    '''\n",
    "    return ((X>=0) + (X<0)*a) * X\n",
    "\n",
    "\n",
    "def convolute(X, w, p_drop):\n",
    "    '''\n",
    "    Performs convolution, ReLU, subsampling and dropout\n",
    "    '''\n",
    "    conv = conv2d(X, w)\n",
    "    rect = rectify(conv)\n",
    "    subs = max_pool2d(rect, (2, 2))\n",
    "    drop = dropout(subs, p_drop)\n",
    "\n",
    "    return drop\n",
    "\n",
    "\n",
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN\n",
    "def cnn_model(x, w_c1, w_c2, w_c3, w_h2, w_a2, w_o, p_drop_input, p_drop_hidden):\n",
    "    c1 = convolute(dropout(x, p_drop_input), w_c1, p_drop_hidden)\n",
    "    c2 = convolute(c1, w_c2, p_drop_hidden)\n",
    "    c3 = convolute(c2, w_c3, p_drop_hidden)\n",
    "\n",
    "    c3 = c3.reshape(-1, 128)\n",
    "\n",
    "    h2 = dropout(c3 @ w_h2, p_drop_hidden)\n",
    "    a2 = PRelu(h2, w_a2)\n",
    "    pre_softmax = a2 @ w_o\n",
    "\n",
    "    return pre_softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time keeping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_to_str(time):\n",
    "    '''\n",
    "    Converts time (in seconds) to more readable format\n",
    "    '''\n",
    "    time = round(time)\n",
    "    hours = time//3600\n",
    "    minutes = (time%3600)//60\n",
    "    seconds = time%60\n",
    "\n",
    "    return f\"{hours:3d}h {minutes:02d}m {seconds:02d}s\"\n",
    "\n",
    "\n",
    "def print_time_info(time_start, time_epoch_start, time, n_epochs, n_drops, i_epoch, i_drop, p_drop):\n",
    "    dtime_total = time - time_start\n",
    "    dtime_epoch = time - time_epoch_start\n",
    "    epochs_done_total = i_drop*n_epochs + i_epoch + 1\n",
    "    epochs_done_drop = i_epoch + 1\n",
    "    epochs_total = n_drops*n_epochs\n",
    "    print(f\"Time elapsed: {time_to_str(dtime_total)} | Time remaining (drop={p_drop}): {time_to_str((n_epochs - epochs_done_drop) * dtime_epoch / epochs_done_drop)} | Time remaining (total): {time_to_str((epochs_total - epochs_done_total) * dtime_total / epochs_done_total)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the training losses are skewed by dropout (since we're multiplying non-dropped inputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stuff for keeping and estimating time\n",
    "time_start = time()\n",
    "n_drops = 3\n",
    "\n",
    "\n",
    "for i, p in enumerate([0.1, 0.25, 0.5]):\n",
    "    print(\"--------\")\n",
    "    print(f\"Running CNN model with dropout {p}\")\n",
    "\n",
    "    time_epoch_start = time()\n",
    "\n",
    "    w_c1 = init_weights((32, 1, 5, 5))\n",
    "    w_c2 = init_weights((64, 32, 5, 5))\n",
    "    w_c3 = init_weights((128, 64, 2, 2))\n",
    "    w_h2 = init_weights((128, 625))\n",
    "    w_a2 = init_weights((625,))\n",
    "    w_o = init_weights((625, 10))\n",
    "\n",
    "    optimizer = RMSprop(params=[w_c1, w_c2, w_c3, w_h2, w_a2, w_o])\n",
    "\n",
    "    n_epochs = 100\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    data[p] = (optimizer, train_loss, test_loss) # for later use\n",
    "\n",
    "    # put this into a training loop over 100 epochs\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        train_loss_this_epoch = []\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            x, y = batch\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # our model requires input of dim (c x w x h)\n",
    "            x = x.reshape(batch_size, 1, 28, 28)\n",
    "            # feed input through model\n",
    "            noise_py_x = cnn_model(x, w_c1, w_c2, w_c3, w_h2, w_a2, w_o, p_drop_input=p, p_drop_hidden=p)\n",
    "\n",
    "            # reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # the cross-entropy loss function already contains the softmax\n",
    "            loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "            train_loss_this_epoch.append(float(loss.to(\"cpu\")))\n",
    "\n",
    "            # compute the gradient\n",
    "            loss.backward()\n",
    "\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "        # test periodically\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "            test_loss_this_epoch = []\n",
    "\n",
    "            # no need to compute gradients for validation\n",
    "            with torch.no_grad():\n",
    "                for idx, batch in enumerate(test_dataloader):\n",
    "                    x, y = batch\n",
    "\n",
    "                    x = x.to(device)\n",
    "                    y = y.to(device)\n",
    "                    x = x.reshape(batch_size, 1, 28, 28)\n",
    "\n",
    "                    # no dropouts for testing!\n",
    "                    noise_py_x = cnn_model(x, w_c1, w_c2, w_c3, w_h2, w_a2, w_o, p_drop_input=0, p_drop_hidden=0)\n",
    "\n",
    "                    loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                    test_loss_this_epoch.append(float(loss.to(\"cpu\")))\n",
    "\n",
    "            test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "            print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "            print_time_info(time_start, time_epoch_start, time(), n_epochs, n_drops, epoch, i, p)\n",
    "\n",
    "print(\"\\nDone!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.gcf()\n",
    "fig.set_size_inches(15, 5)\n",
    "\n",
    "for i, (p, date) in enumerate(data.items()):\n",
    "    _, train_loss, test_loss = date\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.plot(np.arange(1, n_epochs + 2), train_loss, label=f\"Train\")\n",
    "    plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "    plt.title(f\"CNN with drop {p}\")\n",
    "    plt.ylim(0, 3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
