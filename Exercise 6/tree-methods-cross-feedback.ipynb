{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from abc import abstractmethod"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base classes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div style=\"color: green; font-weight:bold\"> The corresponding functions have been implemented correctly. But just as a reminder some constraints for thresholds finding function could be added to make the result accurate, however, for this dataset it doesn't hold.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    '''\n",
    "      this class will later get the following attributes\n",
    "      all nodes:\n",
    "          features\n",
    "          responses\n",
    "      split nodes additionally:\n",
    "          left\n",
    "          right\n",
    "          split_index\n",
    "          threshold\n",
    "      leaf nodes additionally\n",
    "          prediction\n",
    "    '''\n",
    "\n",
    "class Tree:\n",
    "    '''\n",
    "      base class for RegressionTree and ClassificationTree\n",
    "    '''\n",
    "    def __init__(self, n_min=10):\n",
    "        '''n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        self.n_min = n_min \n",
    "    \n",
    "    def predict(self, x):\n",
    "        ''' return the prediction for the given 1-D feature vector x\n",
    "        '''\n",
    "        # first find the leaf containing the 1-D feature vector x\n",
    "        node = self.root\n",
    "        while not hasattr(node, \"prediction\"):\n",
    "            j = node.split_index \n",
    "            if x[j] <= node.threshold:\n",
    "                node = node.left\n",
    "            else:\n",
    "                node = node.right\n",
    "        # finally, return the leaf's prediction\n",
    "        return node.prediction\n",
    "        \n",
    "    def train(self, features, responses, D_try=None):\n",
    "        '''\n",
    "        features: the feature matrix of the training set\n",
    "        response: the vector of responses\n",
    "        '''\n",
    "        N, D = features.shape\n",
    "        assert(responses.shape[0] == N)\n",
    "\n",
    "        if D_try is None:\n",
    "            D_try = int(np.sqrt(D)) # number of features to consider for each split decision\n",
    "        \n",
    "        # initialize the root node\n",
    "        self.root = Node()\n",
    "        self.root.features  = features\n",
    "        self.root.responses = responses\n",
    "\n",
    "        # build the tree\n",
    "        stack = [self.root]\n",
    "        while len(stack):\n",
    "            node = stack.pop()\n",
    "            active_indices = self.select_active_indices(D, D_try)\n",
    "            left, right = self.make_split_node(node, active_indices)\n",
    "            if left is None: # no split found\n",
    "                self.make_leaf_node(node)\n",
    "            else:\n",
    "                stack.append(left)\n",
    "                stack.append(right)\n",
    "    \n",
    "    def make_split_node(self, node, indices):\n",
    "        '''\n",
    "        node: the node to be split\n",
    "        indices: a numpy array of length 'D_try', containing the feature \n",
    "                         indices to be considered for the present split\n",
    "                         \n",
    "        return: None, None -- if no suitable split has been found, or\n",
    "                left, right -- the children of the split\n",
    "        '''\n",
    "        # all responses equal => no improvement possible by any split\n",
    "        if np.unique(node.responses).shape[0] == 1:\n",
    "            return None, None\n",
    "        \n",
    "        # find best feature j_min (among 'indices') and best threshold t_min for the split\n",
    "        l_min = float('inf')  # upper bound for the loss, later the loss of the best split\n",
    "        j_min, t_min = None, None\n",
    "\n",
    "        for j in indices:\n",
    "            thresholds = self.find_thresholds(node, j)\n",
    "\n",
    "            # compute loss for each threshold\n",
    "            for t in thresholds:\n",
    "                loss = self.compute_loss_for_split(node, j, t)\n",
    "\n",
    "                # remember the best split so far \n",
    "                # (the condition is never True when loss = float('inf') )\n",
    "                if loss < l_min:\n",
    "                    l_min = loss\n",
    "                    j_min = j\n",
    "                    t_min = t\n",
    "\n",
    "        if j_min is None: # no split found\n",
    "            return None, None\n",
    "\n",
    "        # create children for the best split\n",
    "        left, right = self.make_children(node, j_min, t_min)\n",
    "\n",
    "        # turn the current 'node' into a split node\n",
    "        # (store children and split condition)\n",
    "        # my code \n",
    "        node.left = left\n",
    "        node.right = right\n",
    "        node.split_index = j_min\n",
    "        node.threshold = t_min\n",
    "\n",
    "        # return the children (to be placed on the stack)\n",
    "        return left, right\n",
    "    \n",
    "    def select_active_indices(self, D, D_try):\n",
    "        ''' return a 1-D array with D_try randomly selected indices from 0...(D-1).\n",
    "        '''\n",
    "        # my code\n",
    "        rand_idx = np.arange(D)\n",
    "        np.random.shuffle(rand_idx)\n",
    "        return rand_idx[:D_try]\n",
    "        \n",
    "        \n",
    "    def find_thresholds(self, node, j):\n",
    "        ''' return: a 1-D array with all possible thresholds along feature j\n",
    "        '''\n",
    "        # my code\n",
    "        sorted = np.sort(node.features[:,j])\n",
    "        thresholds = (sorted[:-1]+ sorted[1:]) /2\n",
    "        # sorts the jth coulumn according to it's value. The threshold is computed as the inbetween values of the sorted values.\n",
    "        return thresholds\n",
    "\n",
    "\n",
    "    def make_children(self, node, j, t):\n",
    "        ''' execute the split in feature j at threshold t\n",
    "        \n",
    "            return: left, right -- the children of the split, with features and responses\n",
    "                                   properly assigned according to the split\n",
    "        '''\n",
    "        left = Node()\n",
    "        right = Node()\n",
    "\n",
    "        # my code\n",
    "        mask_left = node.features[:,j] <= t\n",
    "        mask_right = node.features[:,j] > t\n",
    "        \n",
    "        left.features = node.features[mask_left]\n",
    "        left.responses = node.responses[mask_left]\n",
    "\n",
    "        right.features = node.features[mask_right]\n",
    "        right.responses = node.responses[mask_right]\n",
    "\n",
    "        return left, right\n",
    "        \n",
    "    @abstractmethod\n",
    "    def make_leaf_node(self, node):\n",
    "        ''' Turn node into a leaf by computing and setting `node.prediction`\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''\n",
    "        raise NotImplementedError(\"make_leaf_node() must be implemented in a subclass.\")\n",
    "        \n",
    "    @abstractmethod\n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        ''' Return the resulting loss when the data are split along feature j at threshold t.\n",
    "            If the split is not admissible, return float('inf').\n",
    "        \n",
    "            (must be implemented in a subclass)\n",
    "        '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div style=\"color: green; font-weight:bold\"> Regression tree is implemented correctly, however, instead of calling the len and sum functions and doing some algebra, you could have used mean function  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTree(Tree):\n",
    "    def __init__(self, n_min=1):\n",
    "        super(RegressionTree, self).__init__(n_min)\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        # my code\n",
    "        mask_left = node.features[:,j] <= t\n",
    "        mask_right = node.features[:,j] > t\n",
    "        \n",
    "       \n",
    "        responses_left = node.responses[mask_left]\n",
    "        responses_right = node.responses[mask_right]\n",
    "        \n",
    "        if len(responses_left) < 10 or len(responses_right) < 10: \n",
    "            return float('inf')\n",
    "        \n",
    "        prediction_left = 1/len(responses_left) * np.sum(responses_left)\n",
    "        prediction_right = 1/len(responses_right) * np.sum(responses_right)\n",
    "        loss = np.sum((responses_left-prediction_left)**2) + np.sum((responses_right-prediction_right)**2)\n",
    "        return loss \n",
    "\n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a regression tree is a real number)\n",
    "        # my code\n",
    "        node.prediction = np.mean(node.responses)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div style=\"color: green; font-weight:bold\"> the entropy loss is implemented wringly  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationTree(Tree):\n",
    "    '''implement classification tree so that it can handle arbitrary many classes\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, classes, n_min=10):\n",
    "        ''' classes: a 1-D array with the permitted class labels\n",
    "            n_min: minimum required number of instances in leaf nodes\n",
    "        '''\n",
    "        super(ClassificationTree, self).__init__(n_min)\n",
    "        self.classes = classes\n",
    "        \n",
    "    def compute_loss_for_split(self, node, j, t):\n",
    "        # return the loss if we would split the instance along feature j at threshold t\n",
    "        # or float('inf') if there is no feasible split\n",
    "        mask_left = node.features[:,j] <= t\n",
    "        mask_right = node.features[:,j] > t\n",
    "\n",
    "        responses_left = node.responses[mask_left]\n",
    "        responses_right = node.responses[mask_right]\n",
    "        \n",
    "        if len(responses_left) < 10 or len(responses_right) < 10:\n",
    "            return float('inf')\n",
    "        \n",
    "        n_left_k = np.array([])\n",
    "        n_right_k = np.array([])\n",
    "\n",
    "        for k in np.unique(responses_left):\n",
    "            mask = responses_left == k\n",
    "            np.append(n_left_k,np.sum(mask))\n",
    "        for k in np.unique(responses_right):\n",
    "            mask = responses_right == k\n",
    "            np.append(n_right_k,np.sum(mask))\n",
    "        \n",
    "        N_left = len(responses_left)\n",
    "        N_right = len(responses_right)\n",
    "\n",
    "        p_left_k = n_left_k/N_left\n",
    "        p_right_k = n_right_k/N_right\n",
    "\n",
    "        g_loss = N_left *(1-np.sum(p_left_k**2)) + N_right *(1-np.sum(p_right_k**2))\n",
    "        return g_loss\n",
    "\n",
    "    def make_leaf_node(self, node):\n",
    "        # turn node into a leaf node by computing `node.prediction`\n",
    "        # (note: the prediction of a classification tree is a class label)\n",
    "        # my code\n",
    "        unique_values, counts = np.unique(node.responses, return_counts=True)\n",
    "        max_count_index = np.argmax(counts)\n",
    "        most_frequent_response = unique_values[max_count_index]\n",
    "        node.prediction = most_frequent_response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Classification Tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div style=\"color: green; font-weight:bold\"> The following is correct </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# read and prepare the digits data and extract 3s and 9s\n",
    "digits = load_digits()\n",
    "print(digits.data.shape, digits.target.shape)\n",
    "\n",
    "instances = (digits.target == 3) | (digits.target == 9) \n",
    "features_39 = digits.data[instances, :]\n",
    "labels_39 = digits.target[instances]\n",
    "\n",
    "# for regression, we use labels +1 and -1\n",
    "responses_39 = np.array([1 if l == 3 else -1 for l in labels_39])\n",
    "\n",
    "assert(features_39.shape[0] == labels_39.shape[0] == responses_39.shape[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div style=\"color: green; font-weight:bold\"> It is supposed to calculate the mean of non-matching predictions with the actual responses, therefore, using the difference of them resulted to the wrong solution and the similarity of cross validation for regression and classification could not be reached. </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:  0.2339428322092981 +-  0.07486074312618579\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionTree()\n",
    "# and comment on your results\n",
    "# my code\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validation_regression(model, features, responses, num_sample):\n",
    "    #as the metric we use MAE\n",
    "    absolute_error = np.zeros(num_sample)\n",
    "    k_folds = KFold(n_splits= num_sample)\n",
    "    \n",
    "    for i, (train, test) in enumerate(k_folds.split(features)):\n",
    "        model.train(features[train], responses[train])\n",
    "        predictions = np.array([])\n",
    "        \n",
    "        for f in features[test]:\n",
    "            predictions = np.append(predictions, model.predict(f)) \n",
    "            \n",
    "        absolute_error[i] = 1/(len(predictions)) * np.sum(np.abs(predictions - responses[test]))\n",
    "        \n",
    "    mean_absolute_error = np.mean(absolute_error)\n",
    "    mean_absolute_error_std = np.std(absolute_error)\n",
    "    print('Mean absolute error: ', mean_absolute_error, '+- ', mean_absolute_error_std)\n",
    "\n",
    "cross_validation_regression(RegressionTree(n_min= 10), features_39, responses_39, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error rate:  0.25022831050228306 +- 0.08071622104844442\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using ClassificationTree(classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "# my code\n",
    "def cross_validation_classification(model, features, labels, num_sample):\n",
    "    error_rate = np.zeros(num_sample)\n",
    "    k_folds = KFold(n_splits= num_sample)\n",
    "    \n",
    "    for i, (train, test) in enumerate(k_folds.split(features)):\n",
    "        model.train(features[train], labels[train])\n",
    "        predictions = np.array([])\n",
    "        \n",
    "        for f in features[test]:\n",
    "            predictions = np.append(predictions, model.predict(f))\n",
    "    \n",
    "        error_rate[i] = np.mean(predictions != labels[test])\n",
    "    mean_error_rate = np.mean(error_rate)\n",
    "    std_error_rate = np.std(error_rate)\n",
    "    print('Mean error rate: ', mean_error_rate, '+-', std_error_rate)\n",
    "    \n",
    "cross_validation_classification(ClassificationTree(classes= np.unique(labels_39), n_min= 10), features_39, labels_39, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean error rate fluctuates between (20-40)% since we use different metrics for classification and regression, we can't really compare both results. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Classification Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div style=\"color: green; font-weight:bold\"> This section is implemented correctly</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sampling(features, responses):\n",
    "    '''return a bootstrap sample of features and responses\n",
    "    '''\n",
    "    # my code\n",
    "    # N = features.shape[0]\n",
    "    N = len(features)\n",
    "    i = np.random.randint(0, N, size=N)\n",
    "    return features[i], responses[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RegressionForest():\n",
    "    def __init__(self, n_trees, n_min=10):\n",
    "        # create ensemble\n",
    "        self.trees = [RegressionTree(n_min) for i in range(n_trees)]\n",
    "    \n",
    "    def train(self, features, responses):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, responses)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        # my code\n",
    "        predictions = np.array([])\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            predictions = np.append(predictions, tree.predict(x))\n",
    "        \n",
    "        predictions = np.mean(predictions)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ClassificationForest():\n",
    "    def __init__(self, n_trees, classes, n_min=1):\n",
    "        self.trees = [ClassificationTree(classes, n_min) for i in range(n_trees)]\n",
    "        self.classes = classes\n",
    "    \n",
    "    def train(self, features, labels):\n",
    "        for tree in self.trees:\n",
    "            boostrap_features, bootstrap_responses = bootstrap_sampling(features, labels)\n",
    "            tree.train(boostrap_features, bootstrap_responses)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # compute the response of the ensemble from the individual responses and return it\n",
    "        # my code\n",
    "        predictions = np.array([])\n",
    "        for tree in self.trees:\n",
    "            predictions = np.append(predictions, tree.predict(x))\n",
    "            \n",
    "        unique_values, value_counts = np.unique(predictions, return_counts= True)\n",
    "        most_frequent_index = np.argmax(value_counts)\n",
    "        most_frequent_value = unique_values[most_frequent_index]\n",
    "        return most_frequent_value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Regression and Decision Forest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div style=\"color: green; font-weight:bold\"> There is nothing to say about these parts as they are just recalls of three functions that was explained above how they have been implemented wrongly </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:  0.36974558097598453 +-  0.06854206783839878\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation (see ex01) with responses +1 and -1 (for 3s and 9s)\n",
    "# using RegressionForest(n_trees=10)\n",
    "# and comment on your results\n",
    "# my code\n",
    "cross_validation_regression(RegressionForest(n_trees= 10), features_39, responses_39, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error rate:  0.12408675799086757 +- 0.04110793836588192\n"
     ]
    }
   ],
   "source": [
    "# perform 5-fold cross-validation with labels 3 and 9\n",
    "# using DecisionForest(n_trees=10, classes=np.unique(labels))\n",
    "# and comment on your results\n",
    "# my code\n",
    "cross_validation_classification(ClassificationForest(n_trees= 10, classes= np.unique(labels_39)), features_39, labels_39, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-class Classification Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64) (1797,)\n"
     ]
    }
   ],
   "source": [
    "# Train DecisionForest(n_trees=10, classes=np.unique(digits.target))\n",
    "# for all 10 digits simultaneously.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "# my code\n",
    "digits = load_digits()\n",
    "print(digits.data.shape, digits.target.shape)\n",
    " \n",
    "features_all = digits.data\n",
    "labels_all = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cross_validation_classification(ClassificationForest(n_trees\u001b[39m=\u001b[39;49m \u001b[39m10\u001b[39;49m, classes\u001b[39m=\u001b[39;49m np\u001b[39m.\u001b[39;49munique(labels_all)), features_all, labels_all, \u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m, in \u001b[0;36mcross_validation_classification\u001b[1;34m(model, features, labels, num_sample)\u001b[0m\n\u001b[0;32m      7\u001b[0m k_folds \u001b[39m=\u001b[39m KFold(n_splits\u001b[39m=\u001b[39m num_sample)\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i, (train, test) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(k_folds\u001b[39m.\u001b[39msplit(features)):\n\u001b[1;32m---> 10\u001b[0m     model\u001b[39m.\u001b[39;49mtrain(features[train], labels[train])\n\u001b[0;32m     11\u001b[0m     predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features[test]:\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mClassificationForest.train\u001b[1;34m(self, features, labels)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees:\n\u001b[0;32m      8\u001b[0m     boostrap_features, bootstrap_responses \u001b[39m=\u001b[39m bootstrap_sampling(features, labels)\n\u001b[1;32m----> 9\u001b[0m     tree\u001b[39m.\u001b[39;49mtrain(boostrap_features, bootstrap_responses)\n",
      "Cell \u001b[1;32mIn[2], line 60\u001b[0m, in \u001b[0;36mTree.train\u001b[1;34m(self, features, responses, D_try)\u001b[0m\n\u001b[0;32m     58\u001b[0m node \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39mpop()\n\u001b[0;32m     59\u001b[0m active_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect_active_indices(D, D_try)\n\u001b[1;32m---> 60\u001b[0m left, right \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_split_node(node, active_indices)\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m left \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39m# no split found\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_leaf_node(node)\n",
      "Cell \u001b[1;32mIn[2], line 89\u001b[0m, in \u001b[0;36mTree.make_split_node\u001b[1;34m(self, node, indices)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39m# compute loss for each threshold\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m thresholds:\n\u001b[1;32m---> 89\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss_for_split(node, j, t)\n\u001b[0;32m     91\u001b[0m     \u001b[39m# remember the best split so far \u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[39m# (the condition is never True when loss = float('inf') )\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m loss \u001b[39m<\u001b[39m l_min:\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mClassificationTree.compute_loss_for_split\u001b[1;34m(self, node, j, t)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39munique(responses_left):\n\u001b[0;32m     28\u001b[0m     mask \u001b[39m=\u001b[39m responses_left \u001b[39m==\u001b[39m k\n\u001b[1;32m---> 29\u001b[0m     np\u001b[39m.\u001b[39;49mappend(n_left_k,np\u001b[39m.\u001b[39;49msum(mask))\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39munique(responses_right):\n\u001b[0;32m     31\u001b[0m     mask \u001b[39m=\u001b[39m responses_right \u001b[39m==\u001b[39m k\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Behro\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:5444\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     values \u001b[39m=\u001b[39m ravel(values)\n\u001b[0;32m   5443\u001b[0m     axis \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 5444\u001b[0m \u001b[39mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cross_validation_classification(ClassificationForest(n_trees= 10, classes= np.unique(labels_all)), features_all, labels_all, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<div style=\"color: green; font-weight:bold\"> The last two sections of the code are not implemented at all or correctly, they should be just repalced by the working example and doing some modifications to work with this code. Some parts of the code should be revised completely to be identical to the proper answer of the exercise </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m confusion_matrix_total \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(classes), \u001b[39mlen\u001b[39m(classes)))\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i, (train, test) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(k_folds\u001b[39m.\u001b[39msplit(features_all)):\n\u001b[1;32m---> 10\u001b[0m     model\u001b[39m.\u001b[39;49mtrain(features_all[train], labels_all[train])\n\u001b[0;32m     11\u001b[0m     predictions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m     13\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m features_all[test]:\n",
      "Cell \u001b[1;32mIn[10], line 9\u001b[0m, in \u001b[0;36mClassificationForest.train\u001b[1;34m(self, features, labels)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrees:\n\u001b[0;32m      8\u001b[0m     boostrap_features, bootstrap_responses \u001b[39m=\u001b[39m bootstrap_sampling(features, labels)\n\u001b[1;32m----> 9\u001b[0m     tree\u001b[39m.\u001b[39;49mtrain(boostrap_features, bootstrap_responses)\n",
      "Cell \u001b[1;32mIn[2], line 60\u001b[0m, in \u001b[0;36mTree.train\u001b[1;34m(self, features, responses, D_try)\u001b[0m\n\u001b[0;32m     58\u001b[0m node \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39mpop()\n\u001b[0;32m     59\u001b[0m active_indices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mselect_active_indices(D, D_try)\n\u001b[1;32m---> 60\u001b[0m left, right \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmake_split_node(node, active_indices)\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m left \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m: \u001b[39m# no split found\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_leaf_node(node)\n",
      "Cell \u001b[1;32mIn[2], line 89\u001b[0m, in \u001b[0;36mTree.make_split_node\u001b[1;34m(self, node, indices)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[39m# compute loss for each threshold\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m thresholds:\n\u001b[1;32m---> 89\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss_for_split(node, j, t)\n\u001b[0;32m     91\u001b[0m     \u001b[39m# remember the best split so far \u001b[39;00m\n\u001b[0;32m     92\u001b[0m     \u001b[39m# (the condition is never True when loss = float('inf') )\u001b[39;00m\n\u001b[0;32m     93\u001b[0m     \u001b[39mif\u001b[39;00m loss \u001b[39m<\u001b[39m l_min:\n",
      "Cell \u001b[1;32mIn[4], line 32\u001b[0m, in \u001b[0;36mClassificationTree.compute_loss_for_split\u001b[1;34m(self, node, j, t)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39munique(responses_right):\n\u001b[0;32m     31\u001b[0m     mask \u001b[39m=\u001b[39m responses_right \u001b[39m==\u001b[39m k\n\u001b[1;32m---> 32\u001b[0m     np\u001b[39m.\u001b[39;49mappend(n_right_k,np\u001b[39m.\u001b[39;49msum(mask))\n\u001b[0;32m     34\u001b[0m N_left \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(responses_left)\n\u001b[0;32m     35\u001b[0m N_right \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(responses_right)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Behro\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:5444\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5442\u001b[0m     values \u001b[39m=\u001b[39m ravel(values)\n\u001b[0;32m   5443\u001b[0m     axis \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 5444\u001b[0m \u001b[39mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "classes = np.unique(labels_all)\n",
    "num_sample = 5\n",
    "k_folds = KFold(n_splits= num_sample)\n",
    "model = ClassificationForest(classes= np.unique(labels_all), n_trees= 10)\n",
    "confusion_matrix_total = np.zeros((len(classes), len(classes)))\n",
    "\n",
    "for i, (train, test) in enumerate(k_folds.split(features_all)):\n",
    "    model.train(features_all[train], labels_all[train])\n",
    "    predictions = np.array([])\n",
    "        \n",
    "    for f in features_all[test]:\n",
    "        predictions = np.append(predictions, model.predict(f))\n",
    "        confusion_matrix_total += confusion_matrix(labels_all[test],predictions, labels=classes)\n",
    "average_confusion_matrix = confusion_matrix_total/num_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "im = ax.imshow(average_confusion_matrix, cmap='hot')\n",
    "\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "ax.set_xticks(np.arange(len(classes)))\n",
    "ax.set_yticks(np.arange(len(classes)))\n",
    "ax.set_xticklabels(classes)\n",
    "ax.set_yticklabels(classes)\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "for i in range(len(classes)):\n",
    "    for j in range(len(classes)):\n",
    "        text = ax.text(j, i, int(average_confusion_matrix[i, j]),\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Confusion Matrix\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-against-the-rest classification with RegressionForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ellipsis"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train ten one-against-the-rest regression forests for the 10 digits.\n",
    "# Make sure that all training sets are balanced between the current digit and the rest.\n",
    "# Assign test instances to the digit with highest score, \n",
    "# or to \"unknown\" if all scores are negative.\n",
    "# Compute and plot the confusion matrix after 5-fold cross-validation and comment on your results.\n",
    "... # your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
